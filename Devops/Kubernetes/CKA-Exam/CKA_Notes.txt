PODS

Basic POD Defination yaml file

apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod (name of the pod)
  labels:
    type: frontend
	env: production (Note: the labels can be anything like key value pair)
spec:
  containers:
  - name: nginx-container (name of the container)
    image: nginx

kubectl run <podname> --image=<image name> = To create a container without yaml file

kubectl run nginx-pod --image=nginx

kubectl run nginx-pod --image=nginx --dry-run=client -o yaml = We will directly get the yaml defination file form this

kubectl run nginx-pod --image=nginx --dry-run=client -o yaml > <file name> = We will directly get the yaml defination file form this ans we can save the file with the file name

kubectl create -f <file name> = To create an object mentioned in the file

kubectl get pods = To list the existing pods

kubectl get pods -o wide = To list the pods with full details in CLI itself

kubectl describe pod <podname> = To print the entire details of the pod like container details, event details and conditions

kubectl replace --force -f <defination yaml file> = To delete and recreate the pods (This will be used for updating the pods)

kubectl get pods --watch = It will be used for continuosly monitoring the status of the pods

kubectl get pods --selector <name of the label> = Used to fileter out the pods

kubectl logs pod <pod name> -n <namespace of the pod> = To check the logs of the pod

REPLICASET

Basic Replicaset Defination yaml file

apiVersion: apps/v1
kind: Replicaset
metadata:
  name: nginx-replicaset
spec:
  template:
    metadata:
	  name: nginx-pod1
	  labels:
	    type: frontend
		env: qa
	  spec:
		container:
		- name: nginx-container1
		  image: nginx
  replicas: 4
  selector:
    matchLabels:
	  type: frontend
	  env: qa

kubectl get replicaset = To list the replicaset

kubectl delete replicaset <replicaset name> = To delete the replicaset  Note: It also deletes underlying pods created by itself

kubectl replace -f <replica set defination file name> = To replace the updated changes in the defination file

kubectl scale --replicas=<no of replicas reqd> -f <replica set defination file name> = To scale the replicas without edit the changes in the defination file

kubectl scale rs new-replica-set --replicas=5 = To scale up the replicas

kubectl describe replicaset <replicaset name> = To get the full details about replicaset (Pod template and events)

kubectl edit rs <replicaset name> = To edit the existing replicaset


https://kubernetes.io/docs/reference/kubectl/conventions/


DEPLOYMENTS


Basic Deployment defination file

apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend-deployment
spec:
  replicas: 3
  template:
    metadata:
	  name: apline-pod
	  labels:
	    name: frontend-deployment
	  spec:
	    containers:
		- name: apline-container
		  image: apline:2.1 <change to required image name>
  selector:
    matchLabels:
	  name:: frontend-deployment

kubectl create deployment <deployment name> --image=<image name> --replicas=<no. of replicas> = To create deployment without creating yaml file.

kubectl create deployment <deployment name> --image=<image name> --replicas=<no. of replicas> --dry-run -o yaml = To create the deployment yaml file from the CLI
	 
Note: When we create deployment, deployment will create replication replicaset then replicaset will create pods
	  
kubectl get all = To list all the details related to deployment


SERVICES

we have 3 types of services 

Nodeport service (Range: 30000-32767)
cluster IP
Loadbalancer


Basic service defination yaml (By default if you dont mention the type of service it will create cluster-ip service)

NodePort defination yaml file

apiVersion: v1
kind: Service
metadata:
  name: app-service
spec:
  type: NodePort
  ports:
  - targetPort: 80 (it refers to the pod in which application is running)
    Port: 80 (this referes to service port, Its better to match targetport and port with same i.e is 80 in this case)
	nodePort: 30009 (30000-32767) 
  selector:
    key: value (it refers labels in pod defination file) If any pod matches the label mentioned in the selector, the pod will come under the app-service 


NOTE: the selector in the service defination yaml file targets the labels in the pod defination file. If any pod label will match to the selector mentioned in the service then 
the pod will come under the service

kubectl create service <service name> = To create service without yaml file
kubectl describe service <service name> = To know the full details about the service
kubectl delete service <service name> = To delete the service


NAMESPACES

Basic name space defination file

apiVersion: v1
kind: Namespace
metadata:
  name: <name of the name space>
  labels:
    key1: value1
kubectl create -f <name of the namespace defination file> = To create a namespace from the defination file
kubectl create namespace <name of the namespace> = To create a namespace from the CLI

Basic Pod defination file with namespace mentioned

apiVersion: v1
kind: Pod
metadata:
  name: <pod name>
  namespace: <name of the namespace>
spec:
  containers:
    - name: <container name>
      image: <image name>	

kubectl create -f <pod defincation file> = To create pods as per the namespace mentioned in the pod defination file

kubectl create -f <pod defination file> --namespace=<name of the namespace> = To create pods in a particular namespace without mentioning the namespace under metadata

NOTE: whenever we are creating any resource in the cluster it will be created in default namespace. we can change the namespace by mentioning namespace: <name of the namespace> 
under metadata in defination file.

NOTE: The application running in one namespace can access the backend db running in another namespace  
    "db.service.dev.svc.cluster.local"
	
	here db.service = name of the db in another namespace
	dev = dev namespace
	svc = service
	cluster.local = domain

kubectl get pods --namespace=<name of the name space> = To list the pods in a particular namespace

kubectl config set-context $(kubectl config current-context) --namespace=<name of the namespace to swith> = To switch to the another namespace permanently 
instead of using --namespace=<name of the namespace> always in CLI

kubectl get pods --all-namespaces = To list all the pods available in all the namespaces
kubectl get pods -A = To list all the pods available in all the namespaces

we can limit the resources creating in the namespace by using resource quota

Basic compute-quota yaml file

apiVersion: v1
kind: ResourceQuota
metadata:
  name: <name of the quota>
  namespace: <name of the namespace>
spec:
  hard:
    pods: "10"
	requests.cpu: "4"
	requests.memory: 5Gi
	limits.cpu: "10"
	limits.memory: 10Gi

kubectl create -f <name of the resource quota file>

NOTE: kubectl apply -f <path to the folder where the defination files exists> = To create all the defination files in that path

	
Imperative commands

It will be useful in the exam to save the time
	
https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-service-em-	
https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands

Labels and selectors

it will be using to filter the pods based on the selectors. We can mention labels under metadata in pod defination file and these labels can be referred in selector in 
replicaset/deployment/replication controller

labels should be key and value pair.


Manual scheduling

We can manually schedule the pods by mentioning the nodeName in the pod defination file without depending on the scheduling

apiVersion: v1
kind: Pod
metadata:
  name: <pod name>
  namespace: <name of the namespace>
spec:
  containers:
    - name: <container name>
      image: <image name>
  nodeName: <name of the node>
  

Taints and Tolerations

Taints are applied to nodes
Tolerations are applied to pods

The main use of taints and tolerations is to telling the node to not accept any pod which doesnt have tolerations

kubectl taint nodes <node name> key=value:taint-effect

We do have 3 effects
NoSchedule= which means the scheduler will not schedule any pods on the particular node

PreferNoSchedule = which means the scheduler will not schedule any pods on the particular nodes but we can guaranteed

NoExecute = Which means the scheduler will not schedule the new pods, but the existing pods on the nodes will be evacuated

kubectl taint nodes <node name> key=value:taint-effect- = To delete the taint on the node (NOTE: here added "-" at the end of the schedule to delete the taint)

Basic tolerations pod defination file

apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
spec:
  containers:
  - name: nginx-container
    image: nginx
  tolertations:
  - key: "<it should be matched with the value mentioned in tainted node>"
    value: "<it should be matched with the value mentioned in tainted node>"
	operator: "equal"
	effect: "NoSchedule/PreferNoSchedule/NoExecute"


NODE SELECTOR

By using this option we can schedule the pods on a particular node by using labes on nodes. Till now we have seen labels on pod. Now we will see labels on nodes

To create a label on the node

kubectl label nodes <node name> <label-key>=<label-value> 

kubeclt label nodes node01 size=large

This label we have to mention on the pod under spec

apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
spec:
  containers:
  - name: nginx-container
    image: nginx
  nodeSelector:
    size: large


Now the pod will be created on the node which has label size:large key and value

we have some limitations with the node selector we cant use more labels in the node say example we have 3 nodes and we labelled one as large, one as medium and one as small.
we cant create pod which label has not large. we can't use expressions like or/add in this feature. To do that we have nodeaffinity feature


NODEAFFINITY

In this feature we can ask scheduler to schedule the pods as per the node affinity rules mentioned in the defination file.

Basic node affinity pod defincation file


apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
spec: 
  contianers:
  - name: nginx-container
    image: nginx
  affinity:
    nodeAffinity:
	  requiredDuringSchedulingIgnoredDuringExecution:
	    nodeSelectorTerms:
		- matchExpressions:
		  - key: size
		    operator: In
			value:
			- Large
			- Medium

As per the above node affinity rules mentioned in the pod defination file, we are asking scheduler to schedule the pod if node has a label matching the values 
mentioned in the node affinity rules in pod defination files

we do have 2 types of conditions:

1. requiredDuringSchedulingIgnoredDuringExecution = It means the scheduler will only schedule the pods if the rules mentioned in the pod defination 
file are matched with the labels mentioned in the node. If any existing pods are running in the node, it will continue

2. preferredDuringSchedulingIgnoredDuringExecution = It means the scheduler will schedule the pods even if we are not mentioned the labels in the nodes. 
It will not require the labels, the shceduler will prefer to match the pods with the rules mentioned in the pod defination file with the labels mentioned in the node. If the 
scheduler doesnot find the nodes with out labels it will schedule the pod in any available node. If any existing pods are running in the node, it will continue

3. reuiredDuringSchedulingRequiredDuringExection = It means that the scheduler requires the labels mentioned in the pod defination file, then only it will schedule the pods in 
the nodes matching the node labels orelse the pods will be in pending state. Coming to existing pods running in the node, those pods also should match the node labels as per the 
the affinity rules mentioned in the pod defination file orelase it will be terminated or evacuated 


RESOURCES REQUIREMENTS AND LIMITS

apiVersion: v1
kind: Pod
metadata:
  nginx-pod
spec:
  containers:
  - name: nginx-cont
    image: nginx
  resources: 
    requests:
      memory: "1Gi"
      cpu: 1
	limits:
	  memory: "2Gi"
	  cpu: 2
	  
the above resource requests and limits are only set to containers only

https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/

https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/

https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource


DAEMONSETS

basic demonset defination yaml file

apiVersion: apps/v1
kind: DaemonSet
metadata: 
  name: anyname
spec: 
  template:
    metadata:
	name: nginx-pod
	labels:
	  key: value
  selector:
    matchLabels:
      key: value
  spec:	  
    containers:
    - name: <cont-name>
      image: <imagename>

	NOTE: we can see that we have not mentioned replicas in the daemon set defination file. as it schedule pod in every node in the cluster. id node deletes pod will be deleted.
some example of that pod is monitoring and log collecting pods.

kubectl get daemonsets

kubectl describe daemonset <daemonset name>


STATIC PODS

When we dont have scheduler and api server still we can create pods called static pods by saving the yaml files in the directory mentioned in the kubelet-config file

cd /var/lib/kubelet/config.yaml in which we can find the path with the name called staticPodPath 


MULTIPLE-SCHEDULER

We can use our custom created scheduler to schedule the pods in cluster
we are deploying the additional scheduler as a pod in the cluster

Basic pod defination file to creata a scheduler 

apiVersion: v1
kind: Pod
metadata:
  additional-schedular-pod
spec:
  containers:
  - command:
    - kube-scheduler
	- --address=127.0.0.1
	- --kubeconfig=/etc/kubernetes/scheduler.config
	- --config=/etc/kubernetes/my-scheduler-conf.yaml (we need to create a seperate config file from the default scheduler config file then need to changes as per our need)
	
	image: <image name of the scheduler>
	name: kube-scheduler
	
basic kubernetes scheduler defination yaml file
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
  - schedulerName: multipoint-scheduler
leaderElection: 
  leaderElect: true
  resourceNamespace: kube-system
  
  
basic pod defination file with using custom scheduler 


apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  schedulerName: my-scheduler
  containers:
  - image: nginx
    name: nginx

kubectl get events -o wide = To check the events generated in the namespace

kubectl create configmap <configmap name> --from-file=<path of the file> -n <namespace name>



References for multipe schedulers

https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduling_code_hierarchy_overview.md

https://kubernetes.io/blog/2017/03/advanced-scheduling-in-kubernetes/

https://jvns.ca/blog/2017/07/27/how-does-the-kubernetes-scheduler-work/

https://stackoverflow.com/questions/28857993/how-does-kubernetes-scheduler-work


LOGGIN & MONITORING

we do have different monitoring tools to monitor the kubernetes cluster i.e metrix server. prometheus, datadog, dynatrace

minikube addons enable metrics-server = To enable the metrix server in minikube cluster

To install metrics in the kubeadm cluster, go through the git hub and download all the files and create those in the pods

clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created
serviceaccount/metrics-server created
deployment.apps/metrics-server created
service/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created

kubectl top node = To check the  metrics of the node
kubectl top pod = To check the metrics of the pod


APPLICATION-LOGS

docker logs -f <container id> = To check the live logs of the container

kubectl logs -f <pod name> = To check the live logs of the pod

If multiple containers are running in  a pod

Kubectl logs -f <pod name> <container name> = To check the live logs of a container running inside the pod


ROLLOUT 

kubectl rollout status deployment <deployment name> = To check the deployment rollout status

kubectl rollout history deployment <deployment name> = To check the history of the deployment rollout

types of deployement strategy

recreate starategy = Where are the existing pods will be terminated then only new pods will be created. With this strategy users may face downtime of the application

Rolling update = where one pod is terminated and new pod will be created with the updated version, with this strategy user wont face downtime. This is the default strategy

Suppose if we want to update our image version, we can mention the same in our deployment defination file and apply the changes with kubectl create -f <file name> to make the changes

kubectl set image deployment <deployment name> <container name>:<updated image name with version> = This is also another way to roolout the updates 

we can watch the difference between recreate and rollout updates by using kubectl describe deployment <deployment name> 

In recreate update all the existing pods are scaled down and the new pods will be scaled up wheras in rollout deployment one pod will scale down then new pod will scale up 

While doing any update, existing replicaset will be terminated and new replicaset will be created

kubectl rollout undo deployment <deployment name> = To undo the latest version, this means the new replicaset created will be destroyed and old replicaset will be up with the old verison


kubectl create -f <deployment defination file> = To create a new deployment

kubectl get deployments = To list the deployments

kubectl apply -f <updated deployment defination file> = To deploy the updated deployment as per the changes in the defination file

kubectl set image deployment <deployment name> <container name>:<updated image name with version> = To update the image version without touching the deployment defination file

kubectl rollout status deployment <deployment name> = To check the status of the deployment

kubectl rollout history deployment <deployment name> = To check the history of the deployment

kubectl rollout updo deployement <deployment name> = To undo the rollout updates


for i in {1..35}; do
   kubectl exec --namespace=kube-public curl -- sh -c 'test=`wget -qO- -T 2  http://webapp-service.default.svc.cluster.local:8080/info 2>&1` && echo "$test OK" || echo "Failed"';
   echo ""
done

While changing the strategy type form rollingupdate to recreate make sure to remove the rolling update settings form the yaml file


COMMANDS AND ARGUMENTS

In docker we can override the CMD(command) from the command line but we cannot overwrite the entrypoint

docker run <container name> sleep 10

here sleep is the entrypoint and 10 is the CMD that we can change from the command line like docker run <container name> 20. Everytime we dont need to mention the entrypoint. But if we 
want to change the entrypoint like docker run <container name> sleep2.0 10 

likewise in kubernetes we do have commands and arguments 

Docker   			Kubernetes
Entrypoint 			Command
Command 			Args


BAsic pod defination file with commands and ards

apiVersion: v1
kind: Pod
metadata: 
  name: <name of the pod>
spec: 
  containers:
  -  name: <name of the container>
     image: <image name>
     command: [ "name of the command1", "name of the command2" ] i.e entrypoint in docker
	 args: [ "name of the argument1", "name of the arguement 2" ] i.e command in docker

example of the pod defination file with commands

apiVersion: v1
kind: Pod 
metadata:
  name: ubuntu-sleeper-2
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command:
      - "sleep"
      - "5000"
	 
Pod defination file with args  
apiVersion: v1
kind: Pod
metadata: 
  name: webapp-green
spec: 
  containers:
  - name: anycont
    image: kodekloud/webapp-color
    args:
      - "color"
      - "green"



ENVIRONMENT VARIABLES

basic pod defination file with environment variables

apiVersion: v1
kind: Pod
metadata: 
  name: <name of the pod>
spec:
  containers:
  - image: <name of the image>
    name: <name of the container>
  env:
    - name: APP_COLOUR
      value: pink

in the above file we have hardcoded the value of the environment variable. in some cases we have to grab the value from config map keys and secret keys


env:
  - name: APP_COLOUR
    valueFrom:
      configMapKeyRef


env:
  - name: APP_COLOUR
    valueFrom:
      secretKeyRef


CONFIG MAPS

conig maps are used to pass configuration data in the form of key value pairs. when we have n no of env. variables we can store then in a config file and attach this config file to the pod 
defination file

basic config defination file

apiVersion: v1
kind: configMap
metadata: 
  name: <name of the config map>
date:
  APP_COLOR: blue
  APP_MODE: prod


kubectl apply -f <config file name>


pod defination file with config map

apiVersion: v1
kind: Pod
metadata: 
  name: <name of the pod>
spec:
  containers:
  - envFrom:
    - configMapRef:
        name: webapp-config-map
    image: kodekloud/webapp-color


we can create config maps by using imperative mode

kubectl create configmap <name of the configmap> --from-literal=<key>=<value>

we can add multiple key and values as below

kubectl create configmap <name of the configmap> --from-literal=<key>=<value> \
--from-literal=<key1>=<value1> \
--from-literal=<key2>=<value2> 

we can directl create a configmap from the file

kubectl create configmap <name of the config map> --from-file=<path  of the file>

kubectl get configmaps/cm = To list the config maps

kubectl describe configmaps <configname> = To know the full details of the config map



SECRETS

sometimes we are hardcoding the db username and password in the file that is not good approach. we must pass them via secrets in encoded format

kubectl create secret generic <secret name> --from-literal=<key>=<value> = To create a secret form imperative approach

kubectl create secret generic <secret name> --from-literal=<key>=<value> --from-literal <key1>=<value1> --from-literal <key1>=<value2> = to create multiple key and values at the same time

kubectl create secret generic <secret name> --from-file=<path of the file> = To create a secret from a file

basic secret defination file

apiVersion: v1
kind: Secret
metadata: 
  name: <name of the secret>
data:
  DB_Host: mysql
  DB_USER: Narendra
  DB_PASSSWD: Narendra

kubectl create -f <file name> = to create a secret from the defination file

in the above file we hard hardcoded the secrets in plain text. that is not safe. we can encode the values by using linux terminal by passing the below command

echo -n "secret name> | bas64 = you will get the encoded value and use this value in the secret defination file

kubectl get secrets = To list the secrets

kubeclt describe secrets = To know more info of the secret, but it hides the values of the secret

kubeclt get secret <secret name> -o yaml = To see the values in the secret file 

we can decode the encoded values mentioned in the secret by using linux terminal pasing the below command

echo -n "encoded value" | base64 --decode


Basic pod defination file with secret

apiVersion: v1
kind: Pod
metadata:
  name: <name of the pod>
spec:
  containers:
  - name: <name of the cont>
    image: <name fo the image>
    envFrom:
      - secretRef:
          name: <name of the secret created by the above secret defination file mentiond in metadata>

in the above pod defination file we have injected the whole secret file. Now we will see how to pass a single secret env variable

env:
  - name: DB_Password
    valueFrom: 
      secretKeyRef:
        name: <name of the secret created>
        key: DB_Password 

From the above snippet pod will grab the value of DB_Password from the secret file


if we want to attach secrets as volumes

volumes:
  - name: <name of the secret of the volume>
    secret:
     	secretName: <name of the secret>

each attribute mentioned in the secret will be created as a volume

ls /opt/<name of the secret>/volumes

check the secretvolumes on google


NOTE: secrets can only be encoded not encrypted we can decode that by using the earlier method

secrets are not encrypted in ETCD. how to do that? check below link

https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/

Anyone have access on the namespace to create any objects they can see the secrets

we should create RBAC to control the access 


A note about Secrets!
Remember that secrets encode data in base64 format. Anyone with the base64 encoded secret can easily decode it. As such the secrets can be considered as not very safe.

The concept of safety of the Secrets is a bit confusing in Kubernetes. The kubernetes documentation page and a lot of blogs out there refer to secrets as a "safer option" to store sensitive data. They are safer than storing in plain text as they reduce the risk of accidentally exposing passwords and other sensitive data. In my opinion it's not the secret itself that is safe, it is the practices around it. 

Secrets are not encrypted, so it is not safer in that sense. However, some best practices around using secrets make it safer. As in best practices like:

Not checking-in secret object definition files to source code repositories.

Enabling Encryption at Rest for Secrets so they are stored encrypted in ETCD. 



Also the way kubernetes handles secrets. Such as:

A secret is only sent to a node if a pod on that node requires it.

Kubelet stores the secret into a tmpfs so that the secret is not written to disk storage.

Once the Pod that depends on the secret is deleted, kubelet will delete its local copy of the secret data as well.

Read about the protections and risks of using secrets here



Having said that, there are other better ways of handling sensitive data like passwords in Kubernetes, such as using tools like Helm Secrets, HashiCorp Vault. I hope to make a lecture on these in the future.

 	

INIT CONTAINERS
https://kubernetes.io/docs/concepts/workloads/pods/init-containers/

Kubernetes supports self-healing applications through ReplicaSets and Replication Controllers. The replication controller helps in ensuring that a POD is re-created automatically when the application within the POD crashes. It helps in ensuring enough replicas of the application are running at all times.

Kubernetes provides additional support to check the health of applications running within PODs and take necessary actions through Liveness and Readiness Probes. However these are not required for the CKA exam and as such they are not covered here. These are topics for the Certified Kubernetes Application Developers (CKAD) exam and are covered in the CKAD course.


SELF HEALINH APPLICATIONS
Kubernetes supports self-healing applications through ReplicaSets and Replication Controllers. The replication controller helps in ensuring that a POD is re-created automatically when the application within the POD crashes. It helps in ensuring enough replicas of the application are running at all times.

Kubernetes provides additional support to check the health of applications running within PODs and take necessary actions through Liveness and Readiness Probes. However these are not required for the CKA exam and as such they are not covered here. These are topics for the Certified Kubernetes Application Developers (CKAD) exam and are covered in the CKAD course.


OS -UPGARDES

the time it waits to come back the pod to online is called pro eviction time. we can see that time in controll manager pod
kube-controller-manager --pod-eviction-timeout=5m0s

when we are tying to update any node like os or patch update,  if you know that the pod will be back in 5 mins, then you can do patches and reboot. teh pods running on this nodes 
will be killed, if the pods are created by replicaset, the deleted pod will be created in another node.

besta way yo do updates is like first drain the node

kubectl drain <node name> = To drain the node, so that the pods running on this node will be moved to another nodes. while updation of the node, no pods will be shceduled on the node.
after updation of the node, still no pods will be scheduled untill you uncoden the node

kubectl drain <node name> --ignore-daemonsets = To drain the node if any daemon sets are running

kubectl uncordon <node name> = To uncorden the node

kubectl cordon <node name> = To mark the node as unschedulable, means that no pods will be scheduled and the existing pods will not be termincated and moved to other ndoe


KUBERNETES-RELEASES

go thorought the documentation 

https://kubernetes.io/docs/concepts/overview/kubernetes-api/

Here is a link to kubernetes documentation if you want to learn more about this topic (You don't need it for the exam though):

https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md

https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api_changes.md


CLUSTER UPGRADE PROCESS

kubeadm upgrade plan = It will show you the current version of the master components and availabel update version. kudeadm wont update the kubectl as it is not part of kubeadm.

better practise is to upgrade 1 minor version at a time. ex: your current version is 1.10 available update version is 1.14. We cant directly update from 1.10 to 1.14. we have to update from 1.10 to 1.11 then 1.11 to 1.12 and so on

we have to update kubeadm tool to the version which we want to upgrade to 1 minor version. ex: we have to  update our kubeadm tool first to the version which we want to udpate

apt-get upgrade -y kubeadm=1.12.0-00 = To upgrade the kubeadm tool

kubeadm upgrade apply v1.12.0 = To upgrade our cluster

we can see the output at the kubect get nodes -o wide command. But it will show the old version as the kubelet is registered on these nodes are still the old version  

apt-get upgrade -y kubelet=1.12.0-00 = to upgrade the kubectl

systemctl restart kubelet

we have upgraded the kubelet on master node only. likewise we have to upgrade in the nodes aswell by draining the nodes to move the  workloads to other node

the same process again 

1. kubectl drain <node name> = To move all the workloads to the other node
2. we have to update kubeadmthe required version = apt-get upgrade kubeadm=1.12.0-00
2. we have to upgrade our kubelet = apt-get upgrade kubelet=1.12.0-00
3. kubeadm upgrade node config --kubelet-version v1.12.0
4. systemctl restart kubelet
4. kubectl uncordon <node name>


CLUSTER UPGRADE

follow the below link to upgrade the controlplane and node

https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/


BACKUP & RESTORE

kubectl get all --all-namespaces -o yaml > <name of the file to store all the files>

ETCDCTL_API=3 ectdctl \ snapshot save snapshot.db = To backup the etcd and storing in a file called snapshot.db

ETCDCTL_API=3 etcdctl \ snapshot status snapshot.db = To view the etcd backup

service kube-apiserver stop = To stop the kube-apiserver  (NOTE: Prior restore the etcd from a backup file we should stop the kube-apiserver)

ETCDCTL_API=3 etcdctl \ snapshot restore snapshot.db \ --data-dir <path of the saved file> = To restore all the objects from the backup file

systemctl daemon-reload

service etcd restart 

service kube-apiserver start


NOTE: We need to mention the below certs and endpoints to the file saved

ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 \
> --cacert=/etc/kubernetes/pki/etcd/ca.crt \
> --cert=/etc/kubernetes/pki/etcd/server.crt \
> --key=/etc/kubernetes/pki/etcd/server.key \
> snapshot save /opt/snapshot-pre-boot.db



Use the etcdctl snapshot save command. You will have to make use of additional flags to connect to the ETCD server.

--endpoints: Optional Flag, points to the address where ETCD is running (127.0.0.1:2379)

--cacert: Mandatory Flag (Absolute Path to the CA certificate file)

--cert: Mandatory Flag (Absolute Path to the Server certificate file)

--key: Mandatory Flag (Absolute Path to the Key file)



Working with ETCDCTL

etcdctl is a command line client for etcd.

In all our Kubernetes Hands-on labs, the ETCD key-value database is deployed as a static pod on the master. The version used is v3.

To make use of etcdctl for tasks such as back up and restore, make sure that you set the ETCDCTL_API to 3.

You can do this by exporting the variable ETCDCTL_API prior to using the etcdctl client. This can be done as follows:

export ETCDCTL_API=3


On the Master Node:

To see all the options for a specific sub-command, make use of the -h or --help flag.

For example, if you want to take a snapshot of etcd, use:

etcdctl snapshot save -h and keep a note of the mandatory global options.

Since our ETCD database is TLS-Enabled, the following options are mandatory:

--cacert                                                verify certificates of TLS-enabled secure servers using this CA bundle

--cert                                                    identify secure client using this TLS certificate file

--endpoints=[127.0.0.1:2379]          This is the default as ETCD is running on master node and exposed on localhost 2379.

--key                                                      identify secure client using this TLS key file

Similarly use the help option for snapshot restore to see all available options for restoring the backup.

etcdctl snapshot restore -h

For a detailed explanation on how to make use of the etcdctl command line tool and work with the -h flags, check out the solution video for the Backup and Restore Lab.

Note: In this case, we are restoring the snapshot to a different directory but in the same server where we took the backup (the controlplane node) As a result, the only required option for the restore command is the --data-dir.

Next, update the /etc/kubernetes/manifests/etcd.yaml:

We have now restored the etcd snapshot to a new path on the controlplane - /var/lib/etcd-from-backup, so, the only change to be made in the YAML file, is to change the hostPath for the volume called etcd-data from old directory (/var/lib/etcd) to the new directory (/var/lib/etcd-from-backup).

  volumes:
  - hostPath:
      path: /var/lib/etcd-from-backup
      type: DirectoryOrCreate
    name: etcd-data
With this change, /var/lib/etcd on the container points to /var/lib/etcd-from-backup on the controlplane (which is what we want).

When this file is updated, the ETCD pod is automatically re-created as this is a static pod placed under the /etc/kubernetes/manifests directory.

Note 1: As the ETCD pod has changed it will automatically restart, and also kube-controller-manager and kube-scheduler. Wait 1-2 to mins for this pods to restart. You can run the command: watch "crictl ps | grep etcd" to see when the ETCD pod is restarted.

Note 2: If the etcd pod is not getting Ready 1/1, then restart it by kubectl delete pod -n kube-system etcd-controlplane and wait 1 minute.

Note 3: This is the simplest way to make sure that ETCD uses the restored data after the ETCD pod is recreated. You don't have to change anything else.

If you do change --data-dir to /var/lib/etcd-from-backup in the ETCD YAML file, make sure that the volumeMounts for etcd-data is updated as well, with the mountPath pointing to /var/lib/etcd-from-backup (THIS COMPLETE STEP IS OPTIONAL AND NEED NOT BE DONE FOR COMPLETING THE RESTORE)


IMPPP

kubectl config view = To check the no of cluster in a environment

kubectl config use-context <clustername> = To switch between clusters


References

https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster

https://github.com/etcd-io/website/blob/main/content/en/docs/v3.5/op-guide/recovery.md

https://www.youtube.com/watch?v=qRPNuT080Hk


Security Primitives

WE have to decide 

who can access the cluster and what they can do


AUTHENTICATION

Who can access ?

Files -Username and password
Files - username and tokens
certificates
external authentication providers - LDAP
service accounts

What they can do?

RBAC Authorization
ABAC Authorization
Node Authorization
Webhook Node


AUTHENTICATION to kube-apiserver

Static password file
static token file
certificates
identity services

static password file

In this method we need to create an excel sheet with 3 rows (Password, Username and user id) and the optional is 4 we can assign the user into a group and save the file with .csv extension

this file we need to pass in kube-apiserver service with
--basic-auth-file=<file name>

If you setup your cluster with kubeadm server then go to manifests file /etc/kubernetes/manifests/kube-apiserver.yml file and pass the below line

--basic-auth-file=<file name>


Article on Setting up Basic Authentication
Setup basic authentication on Kubernetes (Deprecated in 1.19)
Note: This is not recommended in a production environment. This is only for learning purposes. Also note that this approach is deprecated in Kubernetes version 1.19 and is no longer available in later releases

Follow the below instructions to configure basic authentication in a kubeadm setup.

Create a file with user details locally at /tmp/users/user-details.csv

# User File Contents
password123,user1,u0001
password123,user2,u0002
password123,user3,u0003
password123,user4,u0004
password123,user5,u0005


Edit the kube-apiserver static pod configured by kubeadm to pass in the user details. The file is located at /etc/kubernetes/manifests/kube-apiserver.yaml



apiVersion: v1
kind: Pod
metadata:
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
      <content-hidden>
    image: k8s.gcr.io/kube-apiserver-amd64:v1.11.3
    name: kube-apiserver
    volumeMounts:
    - mountPath: /tmp/users
      name: usr-details
      readOnly: true
  volumes:
  - hostPath:
      path: /tmp/users
      type: DirectoryOrCreate
    name: usr-details


Modify the kube-apiserver startup options to include the basic-auth file



apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --authorization-mode=Node,RBAC
      <content-hidden>
    - --basic-auth-file=/tmp/users/user-details.csv
Create the necessary roles and role bindings for these users:



---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""] # "" indicates the core API group
  resources: ["pods"]
  verbs: ["get", "watch", "list"]
 
---
# This role binding allows "jane" to read pods in the "default" namespace.
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: User
  name: user1 # Name is case sensitive
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role #this must be Role or ClusterRole
  name: pod-reader # this must match the name of the Role or ClusterRole you wish to bind to
  apiGroup: rbac.authorization.k8s.io
Once created, you may authenticate into the kube-api server using the users credentials

curl -v -k https://localhost:6443/api/v1/pods -u "user1:password123"


TLS

openssl genrsa -out <key name (bank key)> 1024
openssl rsa -in <key name> -pubout <name of the pem (mybank.pem)> 


CERTIFICATE CREATION

we can use different tools to create certificates. Those are EASYRSA, OPENSSL, CFSSL

Now we will look only OPENSSL to create certificates


Steps to create CA Certificate

1. openssl genrsa -out ca.key 2048 = To create a key

2. openssl req -new -key ca.key -subj "/CN=KUBERNETES-CA" -out ca.csr = To create a certificate signing request without sign from the generated key (CN=certificate Name)

3. openssl x509 -req -in ca.csr -signkey ca.key -out ca.cert = To sign on the certificate by using the created key 


ADMIN-USER

1. openssl genrsa -out admin.key 2048 = To create an admin key

2. openssl req -new -key admin.key -subj "CN=KUBE-ADMIN/O=system:masters" -out admin.csr = To create a certificate signing request without sign from the generated key. NOTE: O=system:masters is used to create a group to differentiate kube-admin as other users

3. openssl x509 -req -in admin.csr -CA ca.crt -CAkey ca.key -out admin.crt = To sign on the admin cert with the created CA cert to make admin.cert as valid in the cluster


KUBE_SCHEDULER

1. openssl genrsa -out scheduler.key 2048 = To create an key

2. openssl req -new -key scheduler.key -subj "CN=KUBE-SCHEDULER" -out scheduler.csr = To create a certificate signing request without sign from the generated key. 

3. openssl x509 -req -in scheduler.csr -CA ca.crt -CAkey ca.key -out scheduler.crt = To sign on the cert with the created CA cert to make as valid in the cluster


KUBE_CONTROLLER_MANAGER

1. openssl genrsa -out controller-manager.key 2048 = To create an key

2. openssl req -new -key controller-manager.key -subj "CN=KUBE-CONTROLLER-MANAGER" -out controller-manager.csr = To create a certificate signing request without sign from the generated key. 

3. openssl x509 -req -in controller-manager.csr -CA ca.crt -CAkey ca.key -out controller-manager.crt = To sign on the cert with the created CA cert to make as valid in the cluster


KUBE_CONTROLLER_MANAGER

1. openssl genrsa -out controller-manager.key 2048 = To create an key

2. openssl req -new -key controller-manager.key -subj "CN=KUBE-CONTROLLER-MANAGER" -out controller-manager.csr = To create a certificate signing request without sign from the generated key. 

3. openssl x509 -req -in controller-manager.csr -CA ca.crt -CAkey ca.key -out controller-manager.crt = To sign on the cert with the created CA cert to make as valid in the cluster



KUBE_PROXY

1. openssl genrsa -out kube-proxy.key 2048 = To create an key

2. openssl req -new -key kube-proxy.key -subj "CN=KUBE-PROXY" -out kube-proxy.csr = To create a certificate signing request without sign from the generated key. 

3. openssl x509 -req -in kube-proxy.csr -CA ca.crt -CAkey ca.key -out kube-proxy.crt = To sign on the cert with the created CA cert to make as valid in the cluster



KUBE_API-SERVER

1. openssl genrsa -out apiserver.key 2048 = To create an key

2. openssl req -new -key apiserver.key -subj "CN=KUBE-PROXY" -out apiserver.csr --config openssl.cnf = To create a certificate signing request without sign from the generated key. 

NOTE:
Kube-apiserver has alternate names like

kubernetes
kubernetes.default
kubernetes.default.svc
kubernetes.default.svc.cluster.local
and it has ip address of the host where it is installed or pod ip address where it is running as a pod in the cluster

to pass the above alternate names in the cert, create an openssl.cnf file and specify these names

[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name
[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation,
subjectAltName = @alt_names
[alt_names]
DNS.1 = kubernetes
DNS.2 = kubernetes.default
DNS.3 = kubernetes.default.svc
DNS.4 = kubernetes.default.svc.cluster.local
IP.1 = <Ip address of the pod>
IP.2 = <ip appress of the host, where kube-apiserver installed>

3. openssl x509 -req -in apiserver.csr -CA ca.crt -CAkey ca.key -out apiserver.crt = To sign on the cert with the created CA cert to make as valid in the cluster


VIEW CERTIFICATE DETAILS

It depends on the cluster how you are created.

If you are created cluster by yourself, the path is /etc/systemd/system/

If the cluster is create by kubeadm, the path is /etc/kubernetes/manifests

In the above two paths, we do have services files of the master components. If you open that you can see the path of the each certs.

openssl x509 -in <path of the certificate> -text -noout = To view the content in the cert

journalctl -u etcd.service -l = To list out the logs if cluster is created from scratch

kubectl lods etcd-master = To lsit out the logs if cluster is created by kubeadm

If kube-apiserver and etcd are went down we cant use kubectl to fetch the logs, in that case we can use docker commands

docker ps -a = To list out the containers

docker logs <cont.id> = To check the logs

https://github.com/mmumshad/kubernetes-the-hard-way/tree/master/tools


CERTIFICATES API

till now we have seen only one admin has access the cluster. Now one more admin joined into the group and want access to the cluster. To do that

Create a sshkey for the user with her name
	
	openssl genrsa -out jane.key 2048

sends the key for admin approval

	openssl req -new -key jane.key -subj "/CN=jane" -out jane.csr

then admin need to create yaml file to get the jane.crt to be signed by CA

jane-csr.yaml

apiVersion: certificates.k8s.io/v1beta1
kind: CertificateSigningRequest
metadata: 
  name: jane
spec: 
  groups:
  - system: authenticated
  usages:
  - digital signature
  - key encipherment
  - server auth
  request:
    <paste here the cert file> only in encoded format
     
     echo -n jane-csr | base64 or

	cat jane-csr | base64 | tr -d "/n"

then admin will receive the review request

	kubectl get csr = To list the certificates to approve
	
	kubeclt certificate approve <cert. naem> = To aprove the certificate
	
	kubectl certificate deny <cert.name> = To deny the certificate
	
	kubectl delete csr <csr name> = To delete the csr request

To view the generated cert by the CA
	
	 kubectl get cert <name of the cert (jane)> -o yaml

	decode the certificate as it is in encoded

	echo "cert content" | base64 --decode

finally the can be shared to the new admin

NOTE: all the above is doing by controller manager as it has CSR-APPROVING and CSR-SIGNING in it


KUBE-CONFIG

Basically kube-config file consists of 3 sections

Clusters- where we will mention the list of clusters like dev, predev, stg, prod

Users - the list of users has access on the cluster. The user has different privileges on differnet cluster. ex: prod-user, dev-user, admin

Context - marry clusters and users like which user account will be used to access which cluster

ex: prod-user can access all the cluters in cluster section with his priviliges

    dev-user can access all the cluster in cluster section with his privileges
	
    like admin aswell. It depends on the user privileges. 

NOTE: We are creating any new users. with existing users and their privileges we are accessing different clusters in the cluster section.


Basic kubeconfig file

apiVersion: v1
kind: Config

clusters:
- name: <name of the cluster>
  cluster:
    certificate-authority: <it requires cert. of the cert. authority i.e ca.cert>
    server: <server address>

contexts: 
- name: kube-admin@name of the cluster to link user and cluster
  context:
    cluster: <name of the cluster>
    user: <name of the user, here kube-admin>

users:
- name: <kube-admin>
  user:
    client-certificate: <cert. of the user>
    client-key: <key of the user>


Like the above add the other users and cluster to create contexts

apiVersion: v1
kind: Config
current-context: kube-admin@name of the cluster to link user and cluster

clusters:
- name: <name of the cluster>
  cluster:
    certificate-authority: <it requires cert. of the cert. authority i.e ca.cert>
    server: <server address>

- name: <name of the cluster2>
  cluster:
    certificate-authority: <it requires cert. of the cert. authority i.e ca.cert>
    server: <server address>

- name: <name of the cluster3>
  cluster:
    certificate-authority: <it requires cert. of the cert. authority i.e ca.cert>
    server: <server address>


contexts: 
- name: kube-admin@name of the cluster to link user and cluster
  context:
    cluster: <name of the cluster>
    user: <name of the user, here kube-admin1>

- name: kube-admin2@name of the cluster to link user and cluster
  context:
    cluster: <name of the cluster>
    user: <name of the user, here kube-admin2>

- name: kube-admin3@name of the cluster to link user and cluster
  context:
    cluster: <name of the cluster>
    user: <name of the user, here kube-admin3>


users:
- name: <kube-admin1>
  user:
    client-certificate: <cert. of the user>
    client-key: <key of the user>

- name: <kube-admin2>
  user:
    client-certificate: <cert. of the user>
    client-key: <key of the user>

- name: <kube-admin3>
  user:
    client-certificate: <cert. of the user>
    client-key: <key of the user>

NOTE: Do not create any obeject with this yaml file. Just save it under users/.kube/config

when we have multiple kubeconfig file, save all the kube-config files under the path users/.kube/config
	
kubectl config view = to view the current config file

kubectl config view --kubeconfig=<name of the other kube-config file> = To set the other kubeconfig file as current

kubectl config use-context username@clustername = To change the context in the current kubeconfig file

NOTE: Check the changes were made as per the above command

we can delete/unpdate depends on the other variations like kubectl config -h


When we have multiple namespace in a cluster, what to do?

apiVersion: v1
kind: Config

clusters:
- name: <name of the cluster>
  cluster:
    certificate-authority: <it requires cert. of the cert. authority i.e ca.cert (path of the ca.cert)>/better to use credentials like encoded content
    certificate-authority-data: better to use content in the cert with encoded format 
(NOTE: either we can only use one in the above two options, better to user second one)
    server: <server address>

contexts: 
- name: kube-admin@name of the cluster to link user and cluster
  context:
    cluster: <name of the cluster>
    user: <name of the user, here kube-admin>
    namespace: <name of the namespace in the cluster>

users:
- name: <kube-admin>
  user:
    client-certificate: <cert. of the user>
    client-key: <key of the user>



kubectl config --kubeconfig=my-kube-config use-context research

kubectl config --kubeconfig=my-kube-config current-context = To view the current context


api & api's

core group and named group

core group

api's

v1

namespaces
pods
events
rc
endpoints
nodes
bindings
persistant volume (PV)
persistant volume claims (PVC)
configmaps
secrets 
services


named group

/api's

/apps  --> deployments, replicasets, statefulsets
/extensions
/networking.k8s.io --> /v1  --> /networkpolicies
/storage.k8s.io
/authentication.k8s.io
/certificates/k8s.io  -->/v1  --> CertificateSigningRequest


curl http://localhost:6443 to view the above in the cluster. Sometimes we cannt access the apis's. Then use the certs 

curl http://localhost:6443 -k \
--key <admin key / path of the admin key> \
--cert <admin cert> \
--cacert <cacert path>

kube proxy is not equal to kubelet proxy

kubeproxy is used to enable connectivity between pods and services accross diff nodes in the cluster

kubectl proxy is an httpd proxy service created by kubectl to access the kubeapi server


AUTHORIZATION

node
ABAC
RBAC
webhooks
AlwaysAllow
AlwaysDeny

we can add the AlwaysAllow/AlwaysDeny in the kubeapi server yaml file under authorization-mode. we can define multiple by using comma

RBAC= Role Binding Access Control

https://kubernetes.io/docs/reference/access-authn-authz/rbac/

basic defination file fro rbac

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata: 
  name: developer
rules:
- apiGroups: [""]
  resources: ["Pods"]
  verbs: ["create", "get", "list", delete", "update"]
  resourceName: ["pod name"] = We can restrict the access to developer by giving pod name in the pod list. say you have 10 pods, you want to give access only on 1 pod, you have to mention 
					 that pod name only
- apiGroups: [""]
  resources: ["configMap"]
  verbs: ["create"]

then create an object by applying kubectl apply -f <filename>. We have created a role, next step will be to link the user to the role that we created. 

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata: 
  name: devuser-developer-rolebinding
subjects:
- kind: User
  name: dev-user
  apiGroup: rbac.authorization.k8s.io/v1
- roleRef:
  kind: Role
  name: developer
  apiGroup: rbac.authorization.k8s.io/v1

then create the role binding by using kubectl create -f file name

kubectl get roles = To list the existing roles

kubectl get rolebindings = To list the created rolebindings

kubectl describe role <name of the role> = To see the more details abt it

kubectl describe rolebinding <name of the rolebinding> = To see the more details abt it


kubectl auth can-i create deployments = To check whether you have access to create the deployments

kubectl auth can-i delete nodes = To check whether you have access to delete the nodes

kubectl auth can-i create pods --namespace <name of the namespace> = To check whether the developer can create pods in that name space

kubectl auth can-i create deployments --as dev-user = If admin has created the roles and rolebindings to the dev-user, now we wants to check it configured correctl then he can use this
command to check 


CLUSTER ROLES & CLUSTER ROLE BINDINGS

kubectl api resources --namespaced=true = To see which resources are namespaced

kubectl api resources --namespaced=false = To see which resources are not namespaced (cluster scoped)


To provide access to new admin to only nodes

basic cluster role defination file

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata: 
  name: cluster-admin
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["create", "get", "list", delete", "update"]


basic clusterrolebinding defination file

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata: 
  name: cluster-admin-rolebinding
subjects:
- kind: User
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io/v1
- roleRef:
  kind: ClusterRole
  name: cluster-admin	
  apiGroup: rbac.authorization.k8s.io/v1
  


SERVICE ACCOUNTS

service accounts are used by third party applications like prometheus to pull the logs from K8

kubectl create serviceaccount <service account name> = To create the service account

NOTE: when we create the service account, it will generate token and that token will be stored in secretobject created by itself. This token will be used by the third party application to authenticate

kubectl get serviceaccount = To list the serviceaccounts

kubectl describe serviceaccount <serviceaccount name> = To get the full details and see the token name

kubectl describe secret <secret name in the serviceaccount> = To see the generated token

NOTE: We can't edit service account details in the pod. IF we want so delete the pod and recreate it. by default when we creating any object, it will be used default serviceaccount to mount

basic pod defination file with service account

apiVersion: v1
kind: Pod
metadata:
  name: <name of the pod>
spec:
  containers:
  - name: name of the container>
    image: name of the image
  serviceAccountName: <name of the service account created>

WE can tell to pod to dont mount any serviceaccount to the pod.


basic pod defination file with saying to dont mount any service account

apiVersion: v1
kind: Pod
metadata:
  name: <name of the pod>
spec:
  containers:
  - name: name of the container>
    image: name of the image
  automountServiceAccountToken: false



NOTE: when we are creating any namespace, it will create a serviceaccount itself. when we creating any object under the created namespace, it will use that serviceaccount in that namespace 

we can see the location of the token, the path will be /var/run/secrets/kubernetes.io/serviceaccount. To do that go to the pod by using 

kubectl exec -it <pod name> ls /var/run/secrets/kubernetes.io/serviceaccount = To list the files under service account created

kubectl exec -it <pod name> cat /var/run/secrets/kubernetes.io/serviceaccount/token = To list the files under service account created

we can decode the token by using jwt website. NOTE: there will be no expiry date of the token


Till now we have seen the options till 1.22 version of K8


but from 1.24 version of K8 when we create serviceaccount it will not generate token by itself. we need to create the token by running

kubectl create token <name of the service account> = To create a token and associate to the serviceaccount.

when we creat the token, it will print the content of token and if we decode it jwt wetbsite now it wil have expiry date


IMAGE SECURITY

till now we have seen the images are being pulled form the docker hub public repository. what if we want to use the image in our private repository

say example: when we are pulling the image from docker public repo, we are just giving the name of the image with its tag. But it will consider in this pattern

image: nginx

image: docker.io/library/nginx

docker.io = registry

library = user/account

nginx = image


Now if we want to use the image from private repo

first we need to login withour docker credentials by using docker login, then we should create a secret. This secret will be used by api to pull the image. To create a secret

kubectl create secret docker-registry <name of the secret> \ 
--docker-server= <your server name/DNS name> \
--docker-username= <username> \
--docker-password= <password> \
--docker-email=<email>


we should mention this details in the pod defination file

apiVersion: v1
kind: Pod
metadata: 
  name: nginx
spec: 
  containers:
  - name: nginx-cont
    image: private-repository.io/apps/name of the image
  imagePullSecret:
  - name: name of the secret from the above secret


/usr/share/linux/capability.h  =  To see the list of capabilities of a root user

docker run --cap-add <linux capability> <image name> = To add additional capabilities to the image running in a container

docker run --cap-drop <linux capability> <image name> = To remove the capabilities to the image running in a container

docker run --priviled <image name> = To use all the capabilities to the image running in a container

docker run --user=<userid> ubuntu sleep 3000 = To run the image as a seperate user as the container will be running as a root user by default. to change the user we can add the user id.

NOTE: Root user in the container and the host is not same. The root user in the container doesn't have all the privileges compared with the user in the host

we have seen till now the security at docker level. likewise we can secure in kubernetes also.


apiVersion: v1
kind: Pod
metadata: <name of the pod>
spec:
  containers:
  - name: <name of the cont>
    image: <name of the image>
    command:
    - "sleep"
    - "3000"
    securityContext:
      runAsUser: <user id>
      capabilities:
        add: ["name of the capability1", "name of the capability2"]


kubectl exex <pod name> -- whoami = To check on which user the image is running

NETWORK POLICY

Network policy is nothting but to allow incoming and outgoing traffic to the pods.

lets say there were 3 pods running in a cluster called frontend, api and db pod

user will talk to frontend pod with port 80 and frontend will talk to api pod with 5000port and api talks to db pod on 5432 port. By default in K8 cluster all the pods are communicating with each. This has been done by K8 with AllAllow policy.

Now our intention is that the frondend pod should not communicate with db pod. To do that we have to create an object called network policy.


basic network policy defination file

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata: 
  name: <name of the network policy>
spec:
  podSelector:
    matchLabels:
      label: db-pod (in db pod i have mentioned the label as label: db-pod while creatting db pod)
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector: 
        matchLabels:
          name: api-pod (in api pod i have mentioned the label as label: api-pod while creating api pod)
    ports:
      protocol: TCP
      port: 5432  (this is the only port that you want to allow from api-pod)

NOTE: All the networking system doesnt support network policy obeject to create.

supported networking systems

kube-router
calico
romana
weave-net

un-supported networking systems

flanne

Make sure to check while installing networking system in the cluster

Lets say you rhave 3 namespaces called dev, stg and prod. The api-pods with the labels mentioned as label: api-pod in all the namespaces will talk to the db pod. But it should not happen. Only the api-pods in the prod namespace should talk to db in prod namespace.

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata: 
  name: <name of the network policy>
spec:
  podSelector:
    matchLabels:
      label: db-pod (in db pod i have mentioned the label as label: db-pod while creatting db pod)
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector: 
        matchLabels:
          name: api-pod (in api pod i have mentioned the label as label: api-pod while creating api pod)
	namespaceSelector: (NOTE: if you mentioned the namespace as -namespace, It will impact majorly as the pods with the label as api-pod and all the pods in the particular namespace will 				  talk to the db pod) Make sure prior writing the configuration file..
        matchLabels:
          key1: value1 (namespace label)
    ports:
      protocol: TCP
      port: 5432  (this is the only port that you want to allow from api-pod)

lets say there is a backup db server, which db wants to access for backup, in that case we can use ipblock rule to allow backup server by using port of it


apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata: 
  name: <name of the network policy>
spec:
  podSelector:
    matchLabels:
      label: db-pod (in db pod i have mentioned the label as label: db-pod while creatting db pod)
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector: 
        matchLabels:
          name: api-pod (in api pod i have mentioned the label as label: api-pod while creating api pod)
    -	namespaceSelector: (NOTE: if you mentioned the namespace as -namespace, It will impact majorly as the pods with the label as api-pod and all the pods in the particular namespace will 				  talk to the db pod) Make sure prior writing the configuration file..
        matchLabels:
          key1: value1 (namespace label)
    - ipBlock:
        cidr: 192.168.3.6/16 (ip address of backup server)
  
    ports:
      protocol: TCP
      port: 5432  (this is the only port that you want to allow from api-pod)


Now we want to send the db pod backup to the db server. TO do that we have to install agent in db pod to send the data. There we wil use egress rule as we are sending data to external backup server

NOTE: We can create one network policy for ingress and egress aswell

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: <name of the network policy>
spec:
  podSelector:
    matchLabels:
      label: db-pod
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          name: api-pod
    ports:
    - protocol: TCP
      port: 5000
   egress:
   - to:
     - ipBlock:
        cidr: <ip address of the external backup server>
     ports:
     - protocol: TCP
       port: 80

kubectl get networkpolicy (netpol) = To list the networkpolicies in the cluster

kubectl describe networkpolicy <policy name> = to see the details about policy


Kubectx and Kubens  Command line Utilities
Through out the course, you have had to work on several different namespaces in the practice lab environments. In some labs, you also had to switch between several contexts.



While this is excellent for hands-on practice, in a real live kubernetes cluster implemented for production, there could be a possibility of often switching between a large number of namespaces and clusters.



This can quickly become and confusing and overwhelming task if you had to rely on kubectl alone.



This is where command line tools such as kubectx and kubens come in to picture.



Reference: https://github.com/ahmetb/kubectx



Kubectx:

With this tool, you don't have to make use of lengthy kubectl config commands to switch between contexts. This tool is particularly useful to switch context between clusters in a multi-cluster environment.



Installation:

sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
sudo ln -s /opt/kubectx/kubectx /usr/local/bin/kubectx


Syntax:

To list all contexts:

kubectx



To switch to a new context:

kubectx <context_name>

To switch back to previous context:

kubectx -

To see current context:

kubectx -c

Kubens:

This tool allows users to switch between namespaces quickly with a simple command.

Installation:

sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
sudo ln -s /opt/kubectx/kubens /usr/local/bin/kubens


Syntax:

To switch to a new namespace:

kubens <new_namespace>



To switch back to previous namespace:

kubens -
  
 
STORAGE

Docker storage

Docker builds the images in layers. The layers in the image while building are read only layers and each is dependent on previous layer. But while running running the image into container
the docker will create a layer that is readable and writable.

VOlumes

there were two types of volumes in docker that is bind mount and volume mount

Volume mount will mount the volume from the volumes directory which will be created while creating the volume by running docker volume create data. Here data is the name of the volume.
when we created this volume, automatically a volumes layer will be created at  /var/lib/docker in docker host

Bind mount will mount the external volume anywhere from the host


docker run \
  --mount type=bind,source=/data/mysql,target=/var/lib/mysql sql(image name)

storage drivers are responsible for all the above. there were different kinds of storage drivers

AUFS
ZFS
BTRFS
Device Mapper
Overlay
Overlay2

Depending on the underlying os of the image these storage drivers will be supported. Docker will choose the best storage drivers depends on the base OS

VOlumes are not handled by storage drivers It handled by the volume driver plugins. The default one is local. It helps to create a volume on the docker host and store its data under
/var/lib/docker/volumes directory

some of the docker volume plugins are

Azure file storage
convoy
Digitalocean block storage
flocker
gce-docker
gluster fs
NEtapp
rexray - AWS EBS
portworx
vmsphere vmware storage

docker run -it \
  --name mysql \
  --volume-driver rexray/ebs \
  --mount src=ebs, target=/var/lib/mysql \
  mysql
 
To create a container and attached the volume from AWS cloud. when container exists the data will be stored in the aws cloud.

as we have seen earlier in networking that K8s supports differnt networking systems like flannel, calico, weavenet. This is happening by CNI=container network interface

likewise K8s have CRI=COntainer runtime interface to supports multiple container tools like docker, rocket, cri-o

sameas K8s have CSI = container storage interface which supports multiple storage plugins 

CSI is not a specific K8s standard it meant to be a universal standard and implemented any orcchestration tool work with any storage interface with a supported plugin. Currently K8s, Cloundfoundary and mesos are onbarded with CSI

VOLUMES & MOUNTS

basic pod defination file with volume and mounts

apiVersion: v1
kind: POd
metadata: <name of the pod>
spec:
  containers:
  - name: <namd of the cont>
    image: <name of the image>
    command: ["/bin/sh", "-c"]
    args: ["shuf -i 0-100 -n 1 > /opt/num.out;"]
  volumeMounts:
  - mountPath: /opt
    name: data-vol
  volumes:
  - name: data-vol
    hostPath: 
      path: /data
      type: Directory

We have configured the volume at node level to store the data created by the pod. This is happen when we a single node cluster


If we have multi node cluster, we can store data by creating volume in AWS EBS

volumes:
- name: data-vol
  awsElasticBlockStore:
    volumeID: <volume id>
    fsType: ext4	

PERSISTENT VOLUMES

A persistent volume is a piece of storage in a cluster that an administrator has provisioned. It is a resource in the cluster, just as a node is a cluster resource. A persistent volume is a volume plug-in that has a lifecycle independent of any individual pod that uses the persistent volume. or

A persistent volume is a piece of storage in a cluster that an administrator has provisioned. It is a resource in the cluster, just as a node is a cluster resource. A persistent volume is a volume plug-in that has a lifecycle independent of any individual pod that uses the persistent volume.

basic pv defintion file

apiVersion: v1
kind: PersistantVolume
metadata:
  name: <name of the pv>
spec:
  accessModes: 
  - ReadWriteOnce  NOTE: We do have 3 type of access modes i.e ReadOnlyMany (ROX), ReadWriteOnce (RWO), ReadWriteMany (RWX)
  capacity: 
    storage: 1Gi
  hostPath:
    path: /tmp/data


we can replace the host path with one of the supported storage solutions. Here i am using aws ebs

awsElasticBlockStore:
  volumeId: <id of the ebs>
  fsType: ext4


kubectl create -f <name of the defination file> = to create a persistant volume

kubectl get persistantvolume = To list the volumes


PERSISTENT VOLUME CLAIM

A PersistentVolumeClaim (PVC) is a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access modes (e.g., they can be mounted ReadWriteOnce, ReadOnlyMany or ReadWriteMany, see AccessModes).

defination file of pvc

apiVesion: v1
kind: PersistantVolumeClaim
metadata: 
  name: my-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 500Mi

kubectl create -f <name of the pvc> = To create a PVC from defination file
kubectl get persistantvolumeclaim = To list the pvc
kubectl delete persistantvolumeclaim <name of the pvc> = To delete pvc

What happend to the underlying pv when we deleted pvc. there are 3 policies we can choose

persistantVolumeReclaimPolicy: Retain It means when the pvc is deleted the pv will remain retain untill we delete it manually by the adminstrator. We can reuse this for any other pvc and this is the default policy

persistantVolumeReclaimPolicy: Delete It means when the pvc is deleted the pv will be deleted automatically

persistantVolumeReclaimPolicy: Recycle It is deprecated



Using PVCs in Pods
Once you create a PVC use it in a POD definition file by specifying the PVC Claim name under persistentVolumeClaim section in the volumes section like this:

apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: myclaim


The same is true for ReplicaSets or Deployments. Add this to the pod template section of a Deployment on ReplicaSet.

Reference URL: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#claims-as-volumes


kubectl exec webapp -- cat /log/app.log = TO check the logs in the pod


What are stateful sets?


STORAGE CLASSES

Till now we have seen that we are manually creating the pv. whenever the application is requested pvc claim, we are creating pv manually. This is called static provisioning

Now we will create a pv from the cloud providers in this case GCP, whenever the claim is requested, GCP will automatically create a PV, that pv will be bound to the PVC. This can be used 
by the pod. This is called Dynamic Provision. To do this we have to create disk in google cloud manually. then create a pv with the same name as you have mentioned in the gcp.

To do the dynamic provisioning, we have to create a storage class

storage class defination file

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gcp-sc
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard/pd-ssd
  replication-type: none/regional-pd


kubectl get sc = To list the staorage classes


This storage class should be mentioned in the pvc claim object file

apiVersion: v1
kind: persistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
  - ReadWriteMany
  storageClassName: gcp-sc
  resources:
    requests:
      storage: 500Mi

This pvc should be mentioned in pod defination file

apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: my-pvc

When you created pvc gcp will automatically created the pv with the required size as you are mentioning storageclass in pvc yaml file. but we are not creating pv manually.

There are many other provisioners like aws, azure, cinder, azure file. Based on the type of provision we need to mention the parameters in the storage class defination file.



NETWORKING

iplink = To list and modify the interfaces on the host

ip addr = To see the ipaddress assigned to those interfaces

ip addr add <ip address> dev eht0 = To set ip address to the interface

NOTE: Changes made via this command valid till the restart, if you want to persist them you have to set them in /etc/networkinterfaces file

route / ip route = To see the routing table

ip route add <dest. ip addr> via <route table ip add> = to add entries into the route

To check ip forwarding is enbaled in host machine, if you are working on host to route 

cat /proc/sys/net/ipv4/ip_forward=1 By default this value is 0, to enable ip forwading set it to 1

cat /etc/sysctl.conf


